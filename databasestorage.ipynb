{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  This program stores the textfiles in the database\n",
    "Our cages save every event on a textfile, as an robust and exportable way. Though we also working on a direct database logger, this is currently the easiest way to go with. Mice are free to interact with the system as they want, which produces a lot of different outcomes. The analysis of this amount data is much easier done with a relational database. As first step we upload the raw event data with minimal preprocessing into the database for further anlysis.\n",
    "\n",
    "It might be noted that you need writing permission, so you cannot do this with our database, but you can do this with your own database. Here is guide that might help you to set one up with a phpmyadmin interface which is a pretty nice GUI. You can either using XAMPP or not.\n",
    "\n",
    "https://www.javahelps.com/2018/10/install-mysql-with-phpmyadmin-on-ubuntu.html\n",
    "\n",
    "https://www.youtube.com/watch?v=dV3JjLhi4Jk\n",
    "\n",
    "#### we first import the needed libraries and initialize our database connection function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymysql\n",
    "import glob\n",
    "import re\n",
    "from password import database_password as DBpwd\n",
    "from password import database_user as DBuser\n",
    "from password import database_host as DBhost\n",
    "from password import database as DB\n",
    "\n",
    "def saveToDatabase(query, values):\n",
    "    db1 = pymysql.connect(host=DBhost, user=DBuser, db=DB, password=DBpwd)\n",
    "    cur1 = db1.cursor()\n",
    "    try:\n",
    "        cur1.executemany(query, values)\n",
    "        db1.commit()\n",
    "    except pymysql.Error as e:\n",
    "        try:\n",
    "            print( \"MySQL Error [%d]: %s\" % (e.args[0], e.args[1]))\n",
    "            return None\n",
    "        except IndexError:\n",
    "            print( \"MySQL Error: %s\" % str(e))\n",
    "            return None\n",
    "    except TypeError as e:\n",
    "        print(\"MySQL Error: TypeError: %s\" % str(e))\n",
    "        return None\n",
    "    except ValueError as e:\n",
    "        print(\"MySQL Error: ValueError: %s\" % str(e))\n",
    "        return None\n",
    "    db1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### here we need to make some changes\n",
    "Since there are a lot of events it might be useful to store the events of every cage in different tables for a better overview. The downside is, that it is more complicated to query the extraction of all data at once, since we cannot use a wildcard for table names. We usually just need the raw data for initial analysis, hence this is not a big problem to store each cage in different cages. To get the best from both world we just do both. We start by uploading all data in the same table and mark from which group they are coming from in a `Project_ID` column. Therefore we only have to specify the path where our textfiles are sourced from and we glob them together. TO make sure the code is running it is recommended to keep the folder structure at the end like this ...`\\textfiles\\Group[1-5]_textFiles\\*.txt'`\n",
    "\n",
    "the `[1-5]` is a wildcard for the filename at this part of the path. We read out the project ID automatically due to the folder structure. Please specify the folder path for the `allfiles` variable. If we don't have special notes to make we keep them empty. Having this column gives us the possibility to add some notes later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_commands(vals):\n",
    "    query=\"\"\"INSERT INTO `textfiles`\n",
    "    (`Tag`, `Timestamp`, `Event`, `Project_ID`, `Notes`)\n",
    "        VALUES(%s,FROM_UNIXTIME(%s),%s,%s,%s)\"\"\"\n",
    "    values= vals\n",
    "    return query, values\n",
    "\n",
    "allfiles = glob.glob('D:/Cagedata/textfiles/Group[1-5]_textFiles/*.txt')\n",
    "notes = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### now we upload our textfiles\n",
    "We do some preprocessing. For licks currently the Tag is saved as 000000 and we change that. Also we discovered in some textfiles that some of the reward events are misaligned. We also take care of this problem. We also sort everything by timestamp and drop duplicates\n",
    "we add the project ID to the textfile copy in the DB to introduce a powerful foreign key we might need\n",
    "\n",
    "most powerful keys for this database are timestamps, rfid tags and project ID due to the structure of the textfiles.\n",
    "\n",
    "notes are just for the possibility of manual adding notes later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in allfiles:\n",
    "    df = pd.read_csv(f, sep=\"\\t\",header=None, names = [\"Tag\", \"Unix\", \"Event\", \"Date\"])\n",
    "    print(f)\n",
    "    project = int(''.join([i for i in re.split('_', f) if \"Group\" in i])[-1:])        #careful, requires similar folder structure in the path of allfiles and will change whereever your textfiles are saved                                              \n",
    "\n",
    "    #replace zero tags for the licks\n",
    "    for idx, row in df.iterrows():\n",
    "        if  df.loc[idx,'Event'] == \"SeshStart\" or df.loc[idx,'Event'] == \"SeshEnd\":\n",
    "            df.loc[idx,'Tag'] = \"NULL\"\n",
    "        if df.loc[idx, 'Tag'] == 0:\n",
    "             df.loc[idx, 'Tag'] = df.loc[idx - 1, 'Tag']\n",
    "        if df.loc[idx, 'Date'] == \"reward\":\n",
    "            df.loc[idx, ['Tag', 'Unix', 'Event', 'Date']] = df.loc[idx, ['Tag', 'Event', 'Date', 'Unix']].values\n",
    "    df = df[df.Tag != \"NULL\"]  # kick all rows with Tag: NULL\n",
    "    df = df.sort_values(by=['Unix'])\n",
    "    df = df.drop(labels=\"Date\", axis=1)                             # we don't need the timestamp, the unix timestamp is enough\n",
    "    df.drop_duplicates(['Unix', 'Event'],inplace=True)              # we drop all entries that are\n",
    "    df[\"Project_ID\"] = project                                      # add project ID\n",
    "    df[\"Notes\"] = \"\"                                                # add empty notes column (at least by default)\n",
    "    # we make a list out of it, because that's the easiest way to save it in the DB.\n",
    "    #  there is also a way to directly save pandas Dataframes in the DB\n",
    "    array = df.values.tolist()\n",
    "    print(len(array))\n",
    "    query, values = generate_commands(array)\n",
    "    saveToDatabase(query, values)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can also split up the data into different cage tables if intended.\n",
    "Therefore run the cell below cell with shift-enter after adjusting the target table after INSERT INTO and the source data in the WHERE statement. If this is command is not properly working from python you can run the query directly in your phpmyadmin GUI, there it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"INSERT INTO `textfilesgroup1` (`Tag`,`Timestamp`,`Event`,`Project_ID`,`Notes`) \n",
    "SELECT `Tag`,`Timestamp`,`Event`,`Project_ID`,`Notes` \n",
    "FROM `textfiles` WHERE `Project_ID` = 1\n",
    "ORDER BY `Timestamp` ASC\"\"\"\n",
    "\n",
    "db1 = pymysql.connect(host=DBhost, user=DBuser, db=DB, password=DBpwd)\n",
    "    cur1 = db1.cursor()\n",
    "    try:\n",
    "        cur1.execute(query)\n",
    "        db1.commit()\n",
    "    except pymysql.Error as e:\n",
    "        try:\n",
    "            print( \"MySQL Error [%d]: %s\" % (e.args[0], e.args[1]))\n",
    "            return None\n",
    "        except IndexError:\n",
    "            print( \"MySQL Error: %s\" % str(e))\n",
    "            return None\n",
    "    except TypeError as e:\n",
    "        print(\"MySQL Error: TypeError: %s\" % str(e))\n",
    "        return None\n",
    "    except ValueError as e:\n",
    "        print(\"MySQL Error: ValueError: %s\" % str(e))\n",
    "        return None\n",
    "    db1.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
